# Exploring Livebook and Elixir-Nx

```elixir
Mix.install([
  {:nx, "~> 0.6.4"},
  {:exla, "~> 0.6.4"},
  {:scholar, "~> 0.2.1"},
  {:kino, "~> 0.11.2"},
  {:kino_vega_lite, "~> 0.1.10"},
  {:vega_lite, "~> 0.1.8"},
  {:axon, "~> 0.6"}
])

Nx.global_default_backend(EXLA.Backend)
```

## Introduction

Use linear regression to discover the `Nx` ecosystem.

Linear fitting is a well known convex optimization problem. Under some conditions, it has a unique global solution.

* exact solution: matrix computations with `Nx.LinAlg.invert/1`
* exact solution: statistical formulation in the case univariate
* gradient descent
* simple neural network with `Axon`: perceptron

### Matrix solution

Let $X=(X_1,\dots,X_n)$ be $n$ independant inputs (variables) with corresponding outputs $y=(y_1,\dots, y_n)$. When we have multivariate inputs, each input $X_i$ depends on say $p$ variables. We add $1$ as an extra variable to turn an "affine" problem into a vector problem, and $ ^T$ stands for the transposition operator:
$$
X_i^T = \begin{bmatrix} 1 & x_1^{(i)} & \cdots  & x_p^{(i)}\end{bmatrix} \to y_i
$$

A linear approximation model is the linear map written in matrix form $X^T \cdot W = \hat Y$
$$
\begin{pmatrix}
1       & x_1^{(1)} & \dots   & x_p^{(1)} \\
1       & x_1^{(2)} & \dots   & x_1^{(2)} \\
\vdots  & \vdots    & \ddots  & \vdots \\
1       & x_1^{(N)} & \cdots  & x_p^{(n)}
\end{pmatrix}
\cdot
\begin{bmatrix} w_0 \\ w_1 \\ \vdots \\ w_p\end{bmatrix}

\begin{bmatrix}
\hat y_1 \\
\hat y_2 \\
\vdots \\
\hat y_n
\end{bmatrix}
$$

The approximation is quantified in this case by the **euclidean distance** between the "training" set $T = X \times Y$ and the "prediction" set $X\times \hat Y$. This distance is also designated as "cost" or "loss" function:

$$
C(w) = \| X\cdot W- y\|_2
$$

Let $W^*$ be the minimum  of the map $W \mapsto C(W)$ point, where $\nabla C(W^*)= 0$. When the square matrix $X^TX$  is invertible, the linear system in $W$ can be solved and the exact solution is:

$$
\boxed{\textcolor{blue}W^* = (X^TX)^{-1} X^T\overrightarrow{y}}
$$

#### Analysis of the rank with an example

Imagine we have $n=20$ observations $\overrightarrow{x}^{(i)}=(x^{(1)},\dots, x^{(20)})$, thus $20$ outputs $\overrightarrow{y}_i=(y_1,\dots, y_{20})$ and $p=2$ observables (variables). Recall that we added $1$ so we have $3$ observations. Then $X$ is a $(20\times 3)$ matrix and the equation above is a multiplcation of matrices with the following shapes: 
$$
(3 \times 20)(20 \times 3)(3 \times 20)(20 \times 1) = (3 \times 1)
$$

We implement this formula in `Nx`. We also note that it is of order $O(n^4)$, thus explodes with the number of observations.

<!-- livebook:{"break_markdown":true} -->

#### Getting familiar with building tensors and shapes:

<!-- livebook:{"break_markdown":true} -->

[Create tensors](https://hexdocs.pm/nx/Nx.html#module-creating-tensors). Create a "iota + 1" tensor (`Nx.iota` creates an array of values incrementing from zero)

```elixir
xs =
  Nx.shape({1, 3})
  |> Nx.iota()
  |> Nx.add(1)
  |> dbg()
```

Build a vector with pseudo random values, and add it to our previous vector so that each coordinate is an random number in the corresponding unit interval (ie random between 1 and 2,random between 2 and 3, and random between 3 and 4)

```elixir
t =
  Nx.Random.key(12)
  |> Nx.Random.uniform(0, 0.5, shape: {1, 3}, type: :f64)
  |> elem(0)
  |> dbg()

y_rand = Nx.add(t, xs)
```

Create a vect tensor and transpose it

```elixir
Nx.transpose(xs) |> Nx.shape()
```

Tranpose a vector `Nx.transpose/2`

```elixir
txs = Nx.transpose(xs)
y_rand |> Nx.transpose()
```

Of course, this world is not commutative. Matricial product shape depends on the order. Use `Nx.dot/2`

```elixir
# shape {1,3}x{3,1}={1,1}
Nx.dot(xs, txs) |> Nx.squeeze() |> Nx.to_number() |> dbg()
# shape {3,1}x{1,3} = {3,3}
Nx.dot(txs, xs) |> dbg()
```

We build the "training" data. It is a map `%{x: t[], y: t[]}` where `x` are the $n$ first integers, and `y`is `x` plus some $5$ plus normalized random number.

```elixir
defmodule Training do
  def init(n) do
    x = Nx.shape({1, n}) |> Nx.iota() |> Nx.add(1)

    y =
      Nx.Random.key(12)
      |> Nx.Random.uniform(0, 0.5, shape: {1, n}, type: :f64)
      |> elem(0)
      |> Nx.add(x)
      |> Nx.add(5)
      |> dbg()

    %{x: x, y: y}
  end
end

Training.init(3)
```

##### Exact matricial formula

The function `Nx.LinAlg.invert` does the hard work for us.

```elixir
defmodule Mat do
  import Nx.Defn

  defn inverse(x, y) do
    xt = Nx.transpose(x)

    Nx.dot(xt, x)
    |> Nx.LinAlg.invert()
    |> Nx.dot(xt)
    |> Nx.dot(y)
  end

  def predict(x, y) do
    [[b], [m]] =
      inverse(x, y)
      |> Nx.to_batched(1)
      |> Enum.map(&Nx.to_flat_list/1)

    {b, m}
  end

  defn evaluate({b, m} = _params, t) do
    Nx.dot(m, t) |> Nx.add(b)
  end
end
```

We use the module:

```elixir
n = 25

# creation of fake data randomly along the curve x=y
defmodule Data do
  def training_set(n) do
    r = Enum.to_list(0..(n - 1))

    # training Xs: each vector must have 1 as first element (univariate here)
    c0 = List.duplicate(1, n)

    # build the X tensor with the first column of "1"s
    x =
      Enum.zip(c0, r)
      |> Enum.map(&Tuple.to_list/1)

    # |> Nx.tensor()

    # build y as a list of pseudo random elements.
    y = Enum.map(r, fn x -> x + :rand.uniform() * 2 end)
    # |> Nx.tensor()
    {r, x, y}
  end
end

{r, x, y} = Data.training_set(n)
# calculate W
params = Mat.predict(Nx.tensor(x), Nx.tensor(y))

# check
test_range = [0] ++ Enum.take_random(r, 2) ++ [n]

res =
  Mat.evaluate(params, Nx.tensor(test_range))
  |> Nx.to_batched(1)
  |> Enum.map(&Nx.to_flat_list/1)

# plotting series for vegaLite
trainning_v0 = %{x: r, y: y}
predicted_v0 = %{xp: test_range, yp: Enum.map(res, fn [x] -> x end)}
```

```elixir
VegaLite.new(width: 400, height: 400, title: "Exact solution with inverse matrix")
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(trainning_v0, only: ["x", "y"])
  |> VegaLite.mark(:point)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative),
  VegaLite.new()
  |> VegaLite.data_from_values(predicted_v0, only: ["xp", "yp"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "xp", type: :quantitative)
  |> VegaLite.encode_field(:y, "yp", type: :quantitative)
])
```

### Exact solution with statistical formulas: univariate case

In the case of a univariate linear system (ie $p=1$), we can bypass the matricial inversion. Rewritte $W=[b, w]$. Then the well known exact solution is again just the point where the gradient of the "loss" function is zero. This gives the well known formulas:

$$
\begin{align*}
\textcolor{blue}{w^*} &= \dfrac{\sum (x_i - \bar x)(y_i - \bar y)}{\sum (x_i - \bar x)^2} = \dfrac{\text{cov}(X,Y)}{\text{var}(X)} = \dfrac{\sum_i x_iy_i - n\bar x \bar y}{\sum_i(x-\bar x)^2}
\\
\textcolor{blue}{b^*} &= \bar y - m^* \bar x 
\end{align*}
$$

We can implement this with $\rm Nx$.

```elixir
defmodule Stat do
  import Nx.Defn

  defn(rescale(x), do: x - Nx.mean(x))

  defn cov(x, y) do
    Nx.dot(rescale(x), rescale(y))
    |> Nx.divide(Nx.size(x))
  end

  defn reg(x, y) do
    cov(x, y) |> Nx.pow(2) |> Nx.divide(Nx.variance(y)) |> Nx.divide(Nx.variance(x))
  end

  defn slope(x, y) do
    Nx.divide(cov(x, y), Nx.variance(x))
  end

  defn intercept(x, y) do
    Nx.mean(y) - Nx.multiply(slope(x, y), Nx.mean(x))
  end

  defn predict(x, y, v) do
    intercept(x, y) + Nx.multiply(v, slope(x, y))
  end
end
```

We generate some pseudo random dataset $(x,y)$ and calculate the best fitting lminear map and evaluate the prediction against the dataset on a plot. We check that both solution give the same result :)

```elixir
c0 = List.duplicate(1, n)
# x = Nx.tensor(r)

test_range = [0] ++ Enum.take_random(r, 2) ++ [n]

prediction =
  Stat.predict(Nx.tensor(r), Nx.tensor(y), Nx.tensor(test_range))
  |> Nx.to_batched(1)
  |> Enum.map(&Nx.to_flat_list/1)

training_v1 = %{x: r, y: y}
predicted_v1 = %{xp: test_range, yp: Enum.map(prediction, fn [x] -> x end)}

cr = Stat.reg(Nx.tensor(r), Nx.tensor(y)) |> Nx.to_number()
# Coefficient determination: how well the model fits to the dataset
IO.puts("Coefficient determination: #{cr}")
```

```elixir
VegaLite.new(width: 400, height: 400, title: "Exact solution, statistical formulation")
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(training_v1, only: ["x", "y"])
  |> VegaLite.mark(:point)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative),
  VegaLite.new()
  |> VegaLite.data_from_values(predicted_v1, only: ["xp", "yp"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "xp", type: :quantitative)
  |> VegaLite.encode_field(:y, "yp", type: :quantitative)
])
```

## Gradient descent

In most cases, the solution can only be approached by a convergent process. The incremental modification is the following first order linear transformation proportional to the gradient. `Nx` makes this easy with the `.grad` macro.

$$
L: \begin{pmatrix}
m \\
b
\end{pmatrix}
\mapsto
\begin{pmatrix}
m -\epsilon \partial C / \partial m (m,b) \\
b - \epsilon \partial C / \partial c (m,b)
\end{pmatrix} 
$$

If the scaling factor $ \epsilon$ is too small, the convergence will be slow, and might oscilate or diverge if too big. When there are a lot of variables, computing a whole gradient can be expensive. Computing the partial gradient on a randomly chosen variable only is called stochastic gradient descent. We can even limit the size of the data in batches. Quite a few advanced technics exists.

How do we stop the iterations?

* one can decide to run a fixed number of steps or "epoch", which is mostly used in neural network training. It can also be interesting for a first approach of a problem. It can also overcome non-convex cost functions so that the convergence is not trapped into a local minimum.
* one can decide to stop when the difference in cost between two steps is smaller than a given value $\epsilon$. This is especially useful when the cost function is convex.

```elixir
defmodule GD do
  import Nx.Defn

  defn predict({m, b}, x) do
    Nx.dot(m, x) + b
  end

  def data_to_tensor(data) do
    {x, y} = data |> Enum.unzip()
    {Nx.tensor(x), Nx.tensor(y)}
  end

  defn mse({_m, _b} = params, {x, y} = _dataset) do
    (y - predict(params, x)) |> Nx.pow(2) |> Nx.mean()
  end

  def init(init_params, data) do
    dataset = data_to_tensor(data)
    {mse(init_params, dataset), dataset}
  end

  defn update({m, b} = params, dataset, d) do
    {grad_m, grad_b} = grad(params, fn p -> mse(p, dataset) end)
    {m - grad_m * d, b - grad_b * d}
  end

  defn(diff(new_cost, cost), do: Nx.abs(new_cost - cost))

  # used with reduction to plot Cost=f(count)
  def one_step_obs(params, dataset, d, obs, i) do
    new_params = update(params, dataset, d)
    new_cost = mse(new_params, dataset)
    obs = Map.merge(obs, %{cost: [Nx.to_number(new_cost) | obs.cost], count: [i + 1 | obs.count]})
    {new_params, new_cost, obs}
  end

  # recursion
  def stepping_recursion(params, cost, dataset, epochs, d, eps, obs, count) do
    new_cost = mse(params, dataset)
    {m, b} = update(params, dataset, d)
    # case ((count > epochs) or (Nx.to_number(diff(new_cost, cost)) < eps)) do
    case count > epochs do
      true ->
        {m, b, cost, obs, count}

      false ->
        obs =
          case rem(count, 100) == 0 do
            false ->
              obs

            true ->
              Map.merge(
                obs,
                %{
                  m: [Nx.to_number(m) | obs.m],
                  b: [Nx.to_number(b) | obs.b],
                  cost: [Nx.to_number(new_cost) | obs.cost],
                  count: [count | obs.count]
                }
              )
          end

        stepping_recursion({m, b}, new_cost, dataset, epochs, d, eps, obs, count + 1)
    end
  end

  # reduction loop with data output
  def loop_step(params, dataset, obs, d, count) do
    {m, b} = update(params, dataset, d)
    new_cost = mse({m, b}, dataset)
    count = count + 1

    obs =
      case rem(count, 100) == 0 do
        false ->
          obs

        true ->
          Map.merge(
            obs,
            %{
              m: [Nx.to_number(m) | obs.m],
              b: [Nx.to_number(b) | obs.b],
              cost: [Nx.to_number(new_cost) | obs.cost],
              count: [count | obs.count]
            }
          )
      end

    {{m, b}, obs}
  end
end
```

#### Example

We will set constants `eps`, `epochs` and `d` and an initial point $(m_0,b_0) = (1,0)$ (could be randomized).

We can play with `eps` and `d` and see the impact on the convergence already with theses values. We can have oscillations when the path `d`  is too big or a slow convergence when this values is too small.

<!-- livebook:{"break_markdown":true} -->

We build a pseudo random dataset with $n$ points, in the form $(i, i+ \rm{small random})$

```elixir
n = 25

# pseudo random "training" dataset
r_x = Enum.to_list(0..n)
y_obs = Enum.map(r_x, fn v -> v + 2 + :rand.uniform() end)
x = Nx.tensor(r_x)
y = Nx.tensor(y_obs)
dataset = {Nx.tensor(r_x), Nx.tensor(y_obs)}
data = Enum.zip(r_x, y_obs)
```

We firstly evaluate the values $(m^*, b^*)$ calculated after $n$ steps. It may take some time to complete (no GPU used).

```elixir
d = 0.002
params_0 = {1, 1}
epochs = 5_000
{cost_0, dataset} = GD.init(params_0, data)

{slope, intercept} =
  for _i <- 0..epochs, reduce: params_0 do
    {m, b} ->
      GD.update({m, b}, dataset, d)
  end

IO.puts("slope: #{Nx.to_number(slope)}, intercept: #{Nx.to_number(intercept)}")
GD.mse({slope, intercept}, dataset)
```

Let's build a plot to evaluate the result. We plot the dataset and a line built from a few predicted points.

```elixir
dataset = %{x: r_x, y: y_obs}

x_sample = ([0] ++ Enum.take_random(r_x, 2) ++ [n]) |> IO.inspect()

y_predict =
  Enum.map(
    x_sample,
    fn x -> x * Nx.to_number(slope) + Nx.to_number(intercept) end
  )

Enum.filter(data, fn {x, y} -> if Enum.member?(x_sample, x), do: {x, y} end) |> IO.inspect()

predictions = %{x: x_sample, y: y_predict}
```

<!-- livebook:{"attrs":{"chart_title":null,"height":500,"layers":[{"active":true,"chart_type":"point","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"dataset","geodata_color":"blue","latitude_field":null,"longitude_field":null,"x_field":"x","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"y","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"},{"active":true,"chart_type":"line","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"predictions","geodata_color":"blue","latitude_field":null,"longitude_field":null,"x_field":"x","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"y","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":500},"chunks":null,"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 500, height: 500)
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(dataset, only: ["x", "y"])
  |> VegaLite.mark(:point)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative),
  VegaLite.new()
  |> VegaLite.data_from_values(predictions, only: ["x", "y"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative)
])
```

Let's plot the evolution of the "cost function", the slope and intercept w.r to the number of steps with the gradient descent to understand how the convergence is happening. If we start at $(1,1)$, then we need up to $15000$ steps to reach a plateau. As seen in the graph, the curve $b(steps)$ has an inflexion point, at around 5_000 steps and starts to plateau after 15_000 steps.

```elixir
eps = 0.002
epochs = 10_000
count = 0
obs_0 = %{m: [], b: [], cost: [], count: []}
{cost_0, dataset} = GD.init(params_0, data)

{params, obs} =
  for i <- 0..epochs, reduce: {params_0, obs_0} do
    {params, obs} -> GD.loop_step(params, dataset, obs, d, i)
  end

List.first(obs.cost)
```

```elixir
VegaLite.new()
|> VegaLite.data_from_values(obs, only: ["count", "m", "b", "cost"])
|> VegaLite.concat([
  VegaLite.new()
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "count", type: :quantitative, scale: [type: :log])
  |> VegaLite.encode_field(:y, "m", type: :quantitative),
  VegaLite.new()
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "count", type: :quantitative, scale: [type: :log])
  |> VegaLite.encode_field(:y, "b", type: :quantitative),
  VegaLite.new()
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "count", type: :quantitative, scale: [type: :log])
  |> VegaLite.encode_field(:y, "cost", type: :quantitative, scale: [type: :linear])
])
```

## Simple neural network

We experiment the Axon library to predict values from our dataset. It might seem overkill but it is a simple way to play with tools used in this area with Axon.

A NN is an iterating model or function that combines a linear transformations followed by a non-linear transformation, so-called activation. The goal is again to minimize the "cost" or "loss" function, a simple normalised difference between outputs, by iterating over the function.

Our network is so-called "trained" with a given dataset: from a list of $n$ points $(x_i, y_i)$, we compute the coefficients. We then expect to be able to predict a value for any given $x$.

The output of our univariate model is a single value: an $x$ maps to a single $y$ thus our neural network has a single output. We will feed the NN function with a set of points $\big((x_1, y_1),\cdots,(x_n,y_n))$. Sice we are looking at the linear regression, our activation function will be `:linear`. Then Axon will run an iteration process to computate the coefficients of a linear transform such that it minimizes a loss function. We used the `:mean_squared_error` in the same way as with the gradient descent.

$$
X = (x_1,\dots,x_n) \mapsto y^{(i)} = W \cdot X = \sum_{k=0}^n w_k x_k, \quad \implies \sigma(\sum w_kx_k) \approx y^{(i)}
$$

`output = activation(dot(input, kernel) + bias)`

The algorithm will return a `kernel` and  `bias`.

With Axon, we start by creating a model for the training dataset (the data used to compute the NN): we define the shape of the inputs and define the NN transformations (linear part and activation):

`input = Axon.input("input", shape: {nil, 1})` where  `{nil, 1}` means to input shape is unknown.

The function `Axon.dense(input, 1, activation: :linear)` is composed of a first linear transform followed by an activation function and gives **one** output. We use the linear identity function [`Axon.Activations.linear`](https://hexdocs.pm/axon/Axon.Activations.html#linear/1)

Axon expects data to be in tuples of `{x, y}` where $x$ is model input and $y$ is the target.

We use the loss function `:mean_squared_errors`, the same as in the gradient descent algorithm. We also use the option `compiler: EXLA` which greatly accelerates the computations.

<!-- livebook:{"break_markdown":true} -->

We build the training dataset: is it in the form of points $(i, i+2+\epsilon)$

```elixir
# define training_dataset
defmodule Init do
  def data(n) do
    {
      %{"x" => Nx.tensor(for i <- 1..n, do: [i])},
      Nx.tensor(for i <- 1..n, do: [i + :rand.uniform() + 2])
    }
  end
end
```

```elixir
Init.data(4)
```

```elixir
defmodule NNLR do
  # Define the input and output layers

  def model do
    Axon.input("x", shape: {nil, 1})
    |> Axon.dense(1, activation: :linear)
  end

  def build_model(model) do
    {_init_fn, _predict_fn} = Axon.build(model, compiler: EXLA, mode: :train)
  end

  def train_params(model, training_dataset, epochs, iter) do
    model
    |> Axon.Loop.trainer(:mean_squared_error, :adam)
    |> Axon.Loop.run(training_dataset, %{}, epochs: epochs, iterations: iter, compiler: EXLA)
  end

  def run(training_data, ep, iter) do
    # Axon wants an enumerable or a stream, thus we transform the tuple "{x,y}"
    dataset = Stream.repeatedly(fn -> training_data end)
    _params = model() |> train_params(dataset, ep, iter)
  end

  def predict(model, params, i) do
    Axon.predict(model, params, %{"x" => Nx.tensor([[i]])})
  end

  def graph_model(n) do
    NNLR.model() |> Axon.Display.as_graph(Nx.template({n, 1}, :f32), direction: :top_down)
  end
end
```

```elixir
n = 10

model = NNLR.model() |> dbg()

# run the NN
training_dataset = Init.data(n) |> dbg()
params = NNLR.run(training_dataset, 50, 500) |> dbg()

# extract the coefficients of the predict best linear fit found wth the iterations
%{"dense_0" => %{"bias" => b, "kernel" => m}} = params
[b_nn] = Nx.to_flat_list(b) |> dbg()
[m_nn] = Nx.to_flat_list(m)

NNLR.predict(model, params, n)

# used for the plot

plotter = fn t_d ->
  {%{"x" => x}, y} = t_d
  %{x: Nx.to_flat_list(x), y: Nx.to_flat_list(y)}
end

obs_nn = plotter.(training_dataset)

pred_nn =
  %{x: [0, n], y: [b_nn, m_nn * n + b_nn]}
  |> dbg()

# {init_fn, predict_fn} = NNLR.build_model(model)
# params1 = init_fn.(Nx.template({50,2}, {:f, 32}), %{})
# predict_fn.(params1, %{"x" => x})
# Axon.Display.as_table(model, Nx.template({1, 50}, :f32)) |> IO.puts
```

<!-- livebook:{"attrs":{"chart_title":null,"height":600,"layers":[{"active":true,"chart_type":"point","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"obs_nn","geodata_color":"blue","latitude_field":null,"longitude_field":null,"x_field":"x","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"y","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"},{"active":true,"chart_type":"line","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"pred_nn","geodata_color":"blue","latitude_field":null,"longitude_field":null,"x_field":"x","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"y","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":600},"chunks":null,"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 600, height: 600)
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(obs_nn, only: ["x", "y"])
  |> VegaLite.mark(:point)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative),
  VegaLite.new()
  |> VegaLite.data_from_values(pred_nn, only: ["x", "y"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative)
])
```

```elixir
alias VegaLite, as: Vl

epoch_frame = Kino.Frame.new()

chart =
  Vl.new(width: 400, height: 400)
  |> Vl.data_from_values([])
  |> Vl.encode_field(:x, "iteration",
    type: :quantitative,
    title: "Iteration",
    scale: [domain: [0, 2000]]
  )
  |> Vl.layers([
    Vl.new()
    |> Vl.mark(:line, color: :red)
    |> Vl.encode_field(:y, "loss",
      type: :quantitative,
      scale: [domain: [0, 10]],
      title: "Loss"
    ),
    Vl.new()
    |> Vl.mark(:line, color: :blue)
    |> Vl.encode_field(:y, "accuracy",
      type: :quantitative,
      scale: [domain: [0, 1]],
      title: "Accuracy"
    )
  ])
  |> Vl.resolve(:scale, y: :independent)
  |> Kino.VegaLite.new()

add_chart_data = fn iteration, loss, acc ->
  if rem(iteration, 50) == 0 do
    Kino.VegaLite.push(chart, %{
      iteration: iteration,
      loss: min(Nx.to_number(loss), 10),
      accuracy: Nx.to_number(acc)
    })
  end
end

update_epoch = fn epoch ->
  epoch_frame |> Kino.Frame.render(Kino.Markdown.new("### Epoch #{epoch}"))
end

start_epoch = fn %{epoch: epoch} = state ->
  update_epoch.(epoch)
  Kino.VegaLite.clear(chart)
  {:continue, state}
end

complete_iteration = fn %{metrics: %{"loss" => loss, "accuracy" => acc}, iteration: iteration} =
                          state ->
  add_chart_data.(iteration, Nx.to_number(loss), Nx.to_number(acc))
  {:continue, state}
end

Kino.Frame.render(epoch_frame, Kino.Markdown.new("Training not yet started..."))
Kino.Layout.grid([epoch_frame, chart])
```
