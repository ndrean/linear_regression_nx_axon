# Exploring Livebook and Elixir-Nx

```elixir
Mix.install(
  [
    {:nx, "~> 0.4.0", github: "elixir-nx/nx", sparse: "nx", override: true},
    {:exla, "~> 0.4.1", sparse: "exla"},
    {:scholar, github: "elixir-nx/scholar", override: true},
    {:kino, "~> 0.8.0"},
    {:kino_vega_lite, "~> 0.1.7"},
    {:vega_lite, "~> 0.1.5"},
    {:axon, github: "elixir-nx/axon"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)
```

## Introduction

<a href="https://livebook.dev/run?url=https%3A%2F%2Flin-reg-grad-axon.fly.dev%2Fsessions%2Fjvlktwz5dmariksy4aest5dtmj3b7nfxdte5zvbf">
  <img src="https://livebook.dev/badge/v1/blue.svg" alt="Run in Livebook" />
</a>

<!-- livebook:{"break_markdown":true} -->

Note:

o4isb44pewfb3ixh2jgta4gdzdjjglxs

flyctl secrets set LIVEBOOK_PASSWORD=...

<!-- livebook:{"break_markdown":true} -->

We play with the simple model, the linear regression, as an excuse to start and discover the `Nx` ecosystem of the Elixir-Erlang world, but also Livebook. If needed, some [wiki: linear regression](https://en.wikipedia.org/wiki/Linear_regression) background

Linear fitting is a well known convex optimization problem. Under some conditions, it has a unique global solution.

### Matricial solution

We use the **matricial** primitives of `Nx` with it's package `LinAlg` to calculate it; this gives us the opportunity to check the types or shapes of tensors and use `Nx.transpose/1` and `Nx.LinAlg.invert/1`.

We will also use a root finding algorithm known as the gradient descent and use the handy `.grad` macro.

We will finally - quite pretensiously - setup a kinda neural network to illustrate the usage of the `axon` package.

Now we set up a bit of  notations to conduct the code. Let $X=(X_1,\dots,X_n)$ be $n$ independant inputs (variables) with corresponding outputs $y=(y_1,\dots, y_n)$. When we have multivariate inputs, each input $X_i$ depends on say $p$ variables. We add $1$ as an extra variable to turn an "affine" problem into a vectorial problem:
$$
X_i^T = \begin{bmatrix} 1 & x_1^{(i)} & \cdots  & x_p^{(i)}\end{bmatrix} \to y_i
$$

A linear approximation model is a linear map  with constant terms $m_{i}$ such that each $y_i$ is linearly dependant on $X_i$, and this relation is the same for all $y_i$:
$$

y_i  \approx w_0 + w_{1}x_1^{(i)} + \cdots +w_{p} x_p^{(i)}  = \hat y_i

```
 = X_i^T \cdot W
```

$$
When we consider a univariate model, $y_i$ depends on only one variable $x_1^{(i)}$, so that $y_i \approx w_0 + w_1x_1^{(i)} = \hat y_i$ for each $i\in [1\dots n]$.

The linear model can be rewritten in matrcial form $X^T \cdot W = \hat Y$
$$
\begin{pmatrix}
1       & x_1^{(1)} & \dots   & x_p^{(1)} \\
1       & x_1^{(2)} & \dots   & x_1^{(2)} \\
\vdots  & \vdots    & \ddots  & \vdots \\
1       & x_1^{(N)} & \cdots  & x_p^{(n)}
\end{pmatrix}
\cdot
\begin{bmatrix} w_0 \\ w_1 \\ \vdots \\ w_p\end{bmatrix}

\begin{bmatrix}
\hat y_1 \\
\hat y_2 \\
\vdots \\
\hat y_n
\end{bmatrix}
$$

The approximation is quantified in this case by the **euclidean distance** between the "training" set $T = X \times Y$ and the set $X\times \hat Y$. This distance is also designated as "cost" or "loss" function:

$$
C(w) = \| X\cdot W- y\|_2
$$

As said, there exists a global minimium of the map $W \mapsto C(W)$. Let $W^*$ be this point. At the minimum, the variations of $C$ with respect to $W$ are zero, ie $\nabla C(W^*)= 0$.

When the square matrix $X^TX$  is invertible, the linear system in $W$ can be solved and the exact solution is:

$$
\boxed{\textcolor{blue}W^* = (X^TX)^{-1} X^T\overrightarrow{y}}
$$

#### Analysis of the rank with an example

Imagine we have $n=20$ observations $\overrightarrow{x}^{(i)}=(x^{(1)},\dots, x^{(20)})$, thus $20$ outputs $\overrightarrow{y}_i=(y_1,\dots, y_{20})$ and $p=2$ observables (variables). Recall that we added $1$ so we have $3$ observations. Then $X$ is a $(20\times 3)$ matrix and the equation above is a multiplcation of matrices with the following shapes: 
$$
(3 \times 20)(20 \times 3)(3 \times 20)(20 \times 1) = (3 \times 1)
$$

We implement this formula in `Nx`. We also note that it is of order $O(n^4)$, thus explodes with the number of observations.

<!-- livebook:{"break_markdown":true} -->

#### Getting familiar with building tensors and shapes:

```elixir
range = 1..3
# build a {1,3} "row" tensor: a list of a list
t = [Enum.to_list(range)] |> IO.inspect() |> Nx.tensor()
```

Create a vect tensor and transpose it

```elixir
t = Nx.transpose(t) |> IO.inspect(label: "vect")
Nx.shape(t)
```

Create a vect, name the axis, and tranpose it with `Nx.transpose/2`

```elixir
# build a {3,1} "column tensor aka vector. Equivalently, use Enum.map(& [&1])
vect =
  range
  |> Enum.chunk_every(1)
  |> IO.inspect(label: "chunked")
  |> Nx.tensor(names: [:x, :y])
  |> IO.inspect(label: "vect tensor")
  |> Nx.transpose()
  |> IO.inspect()

Nx.shape(vect)
```

The product shape depends of course on the order. Use `Nx.dot/2`

```elixir
# shape {3,1}x{1,3} = {3,3}
Nx.dot(t, vect) |> IO.inspect()
# shape {1,3}x{3,1}={1,1}
Nx.dot(vect, t)
```

Access rows and columns on the named axis:

```elixir
# check values along a column and rows
{vect[y: 0], for(i <- 0..2, do: vect[x: i])}
```

##### Exact matricial formula

```elixir
defmodule Multi do
  import Nx.Defn

  defn inverse(x, y) do
    xt = Nx.transpose(x)

    Nx.dot(xt, x)
    |> Nx.LinAlg.invert()
    |> Nx.dot(xt)
    |> Nx.dot(y)
  end

  def predict(x, y) do
    [[b], [m]] =
      inverse(x, y)
      |> Nx.to_batched(1)
      |> Enum.map(&Nx.to_flat_list/1)

    {b, m}
  end

  defn evaluate({b, m} = _params, t) do
    Nx.dot(m, t) |> Nx.add(b)
  end
end
```

We use the module:

```elixir
n = 50
r = Enum.to_list(0..(n - 1))

# build y as a list of pseudo random elements
y_init = Enum.map(r, fn x -> x + :rand.uniform() * 10 end)
y = Nx.tensor(y_init)

# training Xs: each vector must have 1 as first element (univariate here)
c0 = List.duplicate(1, n)
# build the X tensor with the first column of "1"s
x =
  Enum.zip(c0, r)
  |> Enum.map(&Tuple.to_list/1)
  |> Nx.tensor()
  |> IO.inspect()

# calculate W
params = Multi.predict(x, y)

# check
test_range = [0] ++ Enum.take_random(r, 2) ++ [n]

res =
  Multi.evaluate(params, Nx.tensor(test_range))
  |> Nx.to_batched(1)
  |> Enum.map(&Nx.to_flat_list/1)

# plotting series for vegaLite
trainning_v0 = %{x: r, y: y_init}
predicted_v0 = %{xp: test_range, yp: Enum.map(res, fn [x] -> x end)}
```

<!-- livebook:{"attrs":{"chart_title":"Exact solution with inverse matrix","height":500,"layers":[{"chart_type":"point","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"trainning_v0","x_field":"x","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"y","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"},{"chart_type":"line","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"predicted_v0","x_field":"xp","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"yp","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":500},"chunks":null,"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 500, height: 500, title: "Exact solution with inverse matrix")
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(trainning_v0, only: ["x", "y"])
  |> VegaLite.mark(:point)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative),
  VegaLite.new()
  |> VegaLite.data_from_values(predicted_v0, only: ["xp", "yp"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "xp", type: :quantitative)
  |> VegaLite.encode_field(:y, "yp", type: :quantitative)
])
```

```elixir
# Enum.reduce(1..(n*p-1), <<0>>, fn _,a -> a <> << 0>> end) |> Nx.from_binary({:u, 8})
```

### Exact solution with statistical formulas: univariate case

In the case of a univariate linear system (ie $p=1$), we can bypass the matricial inversion. Rewritte $W=[b, w]$. Then the exact solution is:

$$
\begin{align*}
\textcolor{blue}{w^*} &= \dfrac{\sum (x_i - \bar x)(y_i - \bar y)}{\sum (x_i - \bar x)^2} = \dfrac{\text{cov}(X,Y)}{\text{var}(X)} = \dfrac{\sum_i x_iy_i - n\bar x \bar y}{\sum_i(x-\bar x)^2}
\\
\textcolor{blue}{b^*} &= \bar y - m^* \bar x 
\end{align*}
$$

We can implement this with $\rm Nx$.

```elixir
defmodule ExactSol do
  import Nx.Defn

  defn(rescale(x), do: x - Nx.mean(x))

  defn cov(x, y) do
    Nx.dot(rescale(x), rescale(y))
    |> Nx.divide(Nx.size(x))
  end

  defn reg(x, y) do
    cov(x, y) |> Nx.power(2) |> Nx.divide(Nx.variance(y)) |> Nx.divide(Nx.variance(x))
  end

  defn slope(x, y) do
    Nx.divide(cov(x, y), Nx.variance(x))
  end

  defn intercept(x, y) do
    Nx.mean(y) - Nx.multiply(slope(x, y), Nx.mean(x))
  end

  defn predict(x, y, v) do
    intercept(x, y) + Nx.multiply(v, slope(x, y))
  end
end
```

We generate some pseudo random dataset $(x,y)$ and calculate the best fitting lminear map and evaluate the prediction against the dataset on a plot. We check that both solution give the same result :)

```elixir
c0 = List.duplicate(1, n)
x = Nx.tensor(r)

test_range = [0] ++ Enum.take_random(r, 2) ++ [n]

prediction =
  ExactSol.predict(x, y, Nx.tensor(test_range))
  |> Nx.to_batched(1)
  |> Enum.map(&Nx.to_flat_list/1)

training_v1 = %{x: r, y: y_init}
predicted_v1 = %{xp: test_range, yp: Enum.map(prediction, fn [x] -> x end)}

# Coefficient determination: how well the model fits to the dataset
IO.puts("Coefficient determination: #{inspect(Nx.to_number(ExactSol.reg(x, y)))}")
```

<!-- livebook:{"attrs":{"chart_title":"Linear regression by exact solution","height":500,"layers":[{"chart_type":"point","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"training_v1","x_field":"x","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"y","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"},{"chart_type":"line","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"predicted_v1","x_field":"xp","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"yp","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":500},"chunks":null,"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 500, height: 500, title: "Linear regression by exact solution")
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(training_v1, only: ["x", "y"])
  |> VegaLite.mark(:point)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative),
  VegaLite.new()
  |> VegaLite.data_from_values(predicted_v1, only: ["xp", "yp"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "xp", type: :quantitative)
  |> VegaLite.encode_field(:y, "yp", type: :quantitative)
])
```

## Gradient descent

In most cases, the solution can only be approached by a convergent process. This is THE tool used is an optimisation process, or "root finding" algorithm.  Consider again the univariate case and a linear map $x \mapsto b + mx$ to approximate a dataset. We modify step by step the point $(m,b)$ such that the function $C(m,b)$  decreases at each step.

At each step, the point will be modified with the linear transformation below of first order:
$$
L: \begin{pmatrix}
m \\
b
\end{pmatrix}
\mapsto
\begin{pmatrix}
m -\epsilon \partial C / \partial m (m,b) \\
b - \epsilon \partial C / \partial c (m,b)
\end{pmatrix} 
$$

A bit more verbose. The curve $C$ behaves like it's gradient locally, so the shifted $L(m,b)$  will remain close to the curve, and closer to the target because we shift it in the opposite direction of the gradient.

The main difficulty is choosing the way to update at each step. We can use  a small multiplier or "scaling factor", say $\epsilon = 0.001$. If the scaling factor is too small, the convergence will be slow, and might oscilate or diverge if too big. When there are a lot of variables, computing a whole gradient can be expensive. Computing the partial gradient on a randomly chosen variable only is called stochastic gradient descent. We can even limit the size of the data in batches. Quite a few advanced technics exists.

How do we stop the iterations?

* one can decide to run a fixed number of steps or "epoch", which is mostly used in neural network training. It can also be interesting for a first approach of a problem. It can also overcome non-convex cost functions so that the convergence is not trapped into a local minimum.
* one can decide to stop when the difference in cost between two steps is smaller than a given value $\epsilon$. This is especially useful when the cost function is convex.

Finally, running the model is running a function that loops continuously until it reaches a stop constraint described above. We can choose run this function recursively or through a reduction loop.

Both methods are illustrated here, the first with the `train` function, and the second with the `step` function. We use the handy macro `.grad` from the package `Nx` to calculate the gradient at each iteration.

```elixir
defmodule GD do
  import Nx.Defn

  defn(predict({m, b}, x), do: Nx.dot(m, x) + b)

  def data_to_tensor(data) do
    {x, y} = data |> Enum.unzip()
    [Nx.tensor(x), Nx.tensor(y)]
  end

  defn cost({m, b} = _params, x, y) do
    (y - Nx.dot(m, x) - b) |> Nx.power(2) |> Nx.mean()
  end

  def init(init_params, data) do
    [x, y] = data_to_tensor(data)
    {cost(init_params, x, y), x, y}
  end

  defn update({m, b} = _params, x, y, d) do
    {grad_m, grad_b} = grad({m, b}, fn {m, b} -> cost({m, b}, x, y) end)
    {m - grad_m * d, b - grad_b * d}
  end

  defn(diff(new_cost, cost), do: Nx.abs(new_cost - cost))

  # used with reduction to plot Cost=f(count)
  def one_step_obs(params, x, y, d, obs, i) do
    {m, b} = update(params, x, y, d)
    new_cost = cost({m, b}, x, y)
    obs = Map.merge(obs, %{cost: [Nx.to_number(new_cost) | obs.cost], count: [i + 1 | obs.count]})
    {m, b, new_cost, obs}
  end

  # recursion
  def stepping_recursion(params, cost, x, y, epochs, d, eps, obs, count) do
    new_cost = cost(params, x, y)
    {m, b} = update(params, x, y, d)
    # case ((count > epochs) or (Nx.to_number(diff(new_cost, cost)) < eps)) do
    case count > epochs do
      true ->
        {params, cost, obs, count}

      false ->
        obs =
          Map.merge(
            obs,
            %{
              m: [Nx.to_number(m) | obs.m],
              b: [Nx.to_number(b) | obs.b],
              cost: [Nx.to_number(new_cost) | obs.cost],
              count: [count | obs.count]
            }
          )

        stepping_recursion({m, b}, new_cost, x, y, epochs, d, eps, obs, count + 1)
    end
  end

  # reduction loop
  def loop_step(params, obs, x, y, d, count) do
    {m, b} = update(params, x, y, d)
    new_cost = cost(params, x, y)
    count = count + 1

    obs =
      Map.merge(
        obs,
        %{
          m: [Nx.to_number(m) | obs.m],
          b: [Nx.to_number(b) | obs.b],
          cost: [Nx.to_number(new_cost) | obs.cost],
          count: [count | obs.count]
        }
      )

    {{m, b}, obs}
  end
end
```

#### Example

We will set constants `eps`, `epochs` and `d` and an initial point $(m_0,b_0) = (1,0)$ (could be randomized).

We can play with `eps` and `d` and see the impact on the convergence already with theses values. We can have oscillations when the path `d`  is too big or a slow convergence when this values is too small.

<!-- livebook:{"break_markdown":true} -->

We build a pseudo random dataset with $n$ points, in the form $(i, i+ \rm{small_random})$

```elixir
n = 50

# pseudo random "training" dataset
r_x = Enum.to_list(1..n)
y_obs = Enum.map(r_x, fn v -> v + :rand.uniform() * 10 end)
data = Enum.zip(r_x, y_obs)
```

We firstly evaluate the values $(m^*, b^*)$ calculated after $n$ steps. It may take some time to complete (no GPU used).

```elixir
d = 0.0011
params_0 = {1, 5}
epochs = 5_000

{slope, intercept} =
  for _i <- 0..epochs, reduce: params_0 do
    {m, b} ->
      GD.update({m, b}, x, y, d)
  end

IO.puts("slope: #{Nx.to_number(slope)}, intercept: #{Nx.to_number(intercept)}")
GD.cost({slope, intercept}, x, y)
```

Let's build a plot to evaluate the result. We plot the dataset and a line built from a few predicted points.

```elixir
dataset = %{x: r_x, y: y_obs}

x_sample = [0] ++ Enum.take_random(r_x, 2) ++ [n]

y_predict =
  Enum.map(
    x_sample,
    fn x -> x * Nx.to_number(slope) + Nx.to_number(intercept) end
  )

predictions = %{x: x_sample, y: y_predict}
```

<!-- livebook:{"attrs":{"chart_title":"Linear regression by gradient descent","height":500,"layers":[{"chart_type":"point","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"dataset","x_field":"x","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"y","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"},{"chart_type":"line","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"predictions","x_field":"x","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"y","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":500},"chunks":null,"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 500, height: 500, title: "Linear regression by gradient descent")
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(dataset, only: ["x", "y"])
  |> VegaLite.mark(:point)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative),
  VegaLite.new()
  |> VegaLite.data_from_values(predictions, only: ["x", "y"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative)
])
```

Let's plot the evolution of the "cost function", the slope and intercept w.r to the number of steps with the gradient descent to understand how the convergence is happening. If we start at $(1,1)$, then we need up to $15000$ steps to reach a plateau. As seen in the graph, the curve $b(steps)$ has an inflexion point, at around 5_000 steps and starts to plateau after 15_000 steps.

```elixir
params_0 = {1, 1}
eps = 0.002
epochs = 15_000
count = 0
obs_0 = %{m: [], b: [], cost: [], count: []}
{cost_0, x, y} = GD.init(params_0, data)

{params, obs} =
  for i <- 0..epochs, reduce: {params_0, obs_0} do
    {params, obs} -> GD.loop_step(params, obs, x, y, d, i)
  end

[c | _] = obs.cost
c
```

```elixir
VegaLite.new()
|> VegaLite.data_from_values(obs, only: ["count", "m", "b", "cost"])
|> VegaLite.concat([
  VegaLite.new()
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "count", type: :quantitative, scale: [type: :log])
  |> VegaLite.encode_field(:y, "m", type: :quantitative),
  VegaLite.new()
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "count", type: :quantitative, scale: [type: :log])
  |> VegaLite.encode_field(:y, "b", type: :quantitative),
  VegaLite.new()
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "count", type: :quantitative, scale: [type: :log])
  |> VegaLite.encode_field(:y, "cost", type: :quantitative, scale: [type: :linear])
])
```

## Simple neural network

We experiment finally the `Axon` librabry to predict values from our dataset. It might seem overkill but it is a simple way to play with tools used in this area with the librabry `Axon`.

A NN is an iterating model or function that combines a linear transformations followed by a non-linear transformation, so-called activation. The goal is again to minimize the "cost" or "loss" function by iterating over the function with again a gradient. All this is abstracted by `Axon` for us.

The output of our univariate model is a single value: an $x$ maps to a single $y$ thus our neural network has a single output. We will feed the NN function with a set of points $\big((x_1, y_1),\cdots,(x_n,y_n))$.

Since we are looking at the linear regression, our activation function will be `:linear`. Then `Axon` will run an iteration process to computate the coefficients of a linear transform such that it minimizes a loss function.
$$
X = (x_1,\dots,x_n) \mapsto y^{(i)} = W \cdot X = \sum_{k=0}^n w_k x_k \to \sigma(\sum w_kx_k) \approx y_{\rm{real}} \quad \implies \text{Cost}(y_{\rm{real}}, y_{\rm{calc}})
$$

`output = activation(dot(input, kernel) + bias)`

The algorithm will return a `kernel` and  `bias`.

With `Axon`, we start by creating a model for the training dataset (the data used to compute the NN): we define the shape of the inputs and define the NN transformations (linear part and activation):

`input = Axon.input("input", shape: {nil, 1})` where  `{nil, 1}` means to input shape is unknown.

Then the [`Axon.dense/4`](https://elixir-nx.github.io/axon/Axon.Layers.html#dense/4) function does exactly this: a linear transform followed by an activation function [`Axon.Activations`](https://elixir-nx.github.io/axon/Axon.Activations.html#summary). We use the linear identity function [`Axon.Activations.linear`](https://hexdocs.pm/axon/Axon.Activations.html#linear/1).
The function `Axon.dense(input, 1, activation: :linear)` does exactly this.

Then is remains to "train" the NN with `Axon.Loop`. We use the loss function `:mean_squared_errors`, the same as in the gradient descent algorithm. We also use the option `compiler: EXLA` which greatly accelerates the computations.

This is based on [this repo](https://github.com/elixir-nx/axon/blob/main/examples/basics/multi_input_example.exs).

Note: `Axon` expects data to be in tuples of `{x, y}` where `x` is in the form  `%{"x" => Nx.tensor([1], [2],...)}` and `y=Nx.tensor([[y1], [y2],...]`.

<!-- livebook:{"break_markdown":true} -->

#### Training dataset

```elixir
# define training_dataset
defmodule Init do
  def data(n) do
    {
      %{"x" => Nx.tensor(for i <- 1..n, do: [i])},
      Nx.tensor(for i <- 1..n, do: [i + :rand.uniform() * 5])
    }
  end
end
```

#### implementation of the NN

```elixir
defmodule NNLR do
  # Define the input and output layers
  def model do
    _model =
      Axon.input("x", shape: {nil, 1})
      |> Axon.dense(1, activation: :linear)
  end

  def build_model(model) do
    {_init_fn, _predict_fn} = Axon.build(model, compiler: EXLA, mode: :train)
  end

  def train_params(model, training_dataset, epochs, iter) do
    model
    |> Axon.Loop.trainer(:mean_squared_error, :adam)
    |> Axon.Loop.run(training_dataset, %{}, epochs: epochs, iterations: iter, compiler: EXLA)
  end

  def run(training_data, opts \\ []) do
    ep = Keyword.get(opts, :ep, 5)
    iter = Keyword.get(opts, :iter, 1000)
    # Axon wants an enumerable or a stream, thus we transform the tuple "{x,y}"
    dataset = Stream.repeatedly(fn -> training_data end)
    _params = model() |> train_params(dataset, ep, iter)
  end

  def predict(model, params, i) do
    Axon.predict(model, params, %{"x" => Nx.tensor([[i]])})
  end
end
```

#### Running the NN model

```elixir
training_dataset = Init.data(50)

model = NNLR.model()
# run the NN
params = NNLR.run(training_dataset)

# extract the coefficients of the predict best linear fit found wth the iterations
%{"dense_0" => %{"bias" => b, "kernel" => m}} = params
IO.inspect([b_nn] = Nx.to_flat_list(b), label: "intercept")
IO.inspect([m_nn] = Nx.to_flat_list(m), label: "slope")

NNLR.predict(model, params, 5)
|> Nx.to_flat_list()
|> IO.inspect(label: "valeur finale")

# used for the plot
plotter = fn t_d ->
  {%{"x" => x}, y} = t_d
  %{x: Nx.to_flat_list(x), y: Nx.to_flat_list(y)}
end

obs_nn = plotter.(training_dataset)

pred_nn =
  %{x: [0, n], y: [b_nn, m_nn * n + b_nn]}
  |> IO.inspect()

NNLR.model() |> Axon.Display.as_graph(Nx.template({n, 1}, :f32), direction: :top_down)

# not working yet
{init_fn, predict_fn} = NNLR.build_model(model)
# params1 = init_fn.(Nx.template({1, 1}, {:f, 32}), %{})
# init_fn.(Nx.to_template(model),params)
# predict_fn.(params1, %{"x" => x})
```

<!-- livebook:{"attrs":{"chart_title":"Linear regression by NN","height":600,"layers":[{"chart_type":"point","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"obs_nn","x_field":"x","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"y","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"},{"chart_type":"line","color_field":null,"color_field_aggregate":null,"color_field_bin":false,"color_field_scale_scheme":null,"color_field_type":null,"data_variable":"pred_nn","x_field":"x","x_field_aggregate":null,"x_field_bin":false,"x_field_scale_type":null,"x_field_type":"quantitative","y_field":"y","y_field_aggregate":null,"y_field_bin":false,"y_field_scale_type":null,"y_field_type":"quantitative"}],"vl_alias":"Elixir.VegaLite","width":600},"chunks":null,"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
VegaLite.new(width: 600, height: 600, title: "Linear regression by NN")
|> VegaLite.layers([
  VegaLite.new()
  |> VegaLite.data_from_values(obs_nn, only: ["x", "y"])
  |> VegaLite.mark(:point)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative),
  VegaLite.new()
  |> VegaLite.data_from_values(pred_nn, only: ["x", "y"])
  |> VegaLite.mark(:line)
  |> VegaLite.encode_field(:x, "x", type: :quantitative)
  |> VegaLite.encode_field(:y, "y", type: :quantitative)
])
```

```elixir
:erlang.term_to_binary(1 + :rand.uniform())
|> :erlang.binary_to_term()
```
